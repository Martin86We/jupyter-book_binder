{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108421eb-0adc-4fb1-949b-215ca2692466",
   "metadata": {},
   "source": [
    "(cnn)=\n",
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d600ec1-b83b-4590-b787-e5ec759d1f2d",
   "metadata": {},
   "source": [
    "**Weshalb Convolutional Neural Networks?**\n",
    "Das Hauptstrukturmerkmal von neuronalen Netzen ist, dass alle Neuronen miteinander verbunden sind. Wenn wir beispielsweise Bilder mit 28 x 28 Pixeln in Graustufen haben, haben wir am Ende 784 (28 x 28 x 1) Neuronen in einer Ebene, was überschaubar erscheint. Die meisten Bilder haben jedoch viel mehr Pixel und sind nicht grau skaliert. Unter der Annahme, dass eine Reihe von Farbbildern in 4K Ultra HD vorliegt, haben wir also 26.542.080 (4096 x 2160 x 3) verschiedene Neuronen, die in der ersten Schicht miteinander verbunden sind, was nicht wirklich handhabbar ist. Daher können wir sagen, dass neuronale Netze ohne Convolutional Layer, für die Bildklassifizierung nicht skalierbar sind. Insbesondere bei Bildern scheint es jedoch wenig Korrelation oder Beziehung zwischen zwei einzelnen Pixeln zu geben, es sei denn, sie liegen nahe beieinander. Dies führt zu der Idee von Convolutional Layers und Pooling Layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eec8b4-51d0-4c5e-b5e5-13756c5d3f69",
   "metadata": {},
   "source": [
    "Ein Theorem aus dem Jahr 1988, das \"Universal Approximation Theorem\", sagt, dass jede beliebige, glatte Funktion, durch ein NN mit nur einem Hidden Layer approximiert werden kann. Nach diesem Theorem, würde dieses einfache NN bereits in der Lage sein, jedes beliebige Bild bzw. die Funktion der Pixelwerte zu erlernen. Die Fehler und die lange Rechenzeit zeigen die Probleme in der Praxis. Denn um dieses Theorem zu erfüllen, sind für sehr einfache Netze unendlich viel Rechenleistung, Zeit und Trainingsbeispiele nötig. Diese stehen i.$~$d.$~$R. nicht zur Verfügung. Für die Bilderkennung haben sich CNNs als sehr wirksam erwiesen. Die Arbeitsweise soll in diesem Abschnitt erläutert werden.\n",
    "Der Grundgedanke bei der Nutzung der Convolutional Layer ist, dem NN zusätzliches „Spezialwissen“  über die Daten zu geben. Das NN ist durch den zusätzlichen Convolutional Layer in der Lage, spezielle Bildelemente und Strukturen besser zu erkennen. \n",
    "\n",
    "Es werden meist mehrere Convolutional Layer hintereinander geschalten. Das NN kann auf der ersten Ebene lernen, Kanten zu erkennen. Auf weiteren Ebenen lernt es dann weitere \"Bild-Features\" wie z.$~$B. Übergänge, Rundungen o.$~$ä. zu erkennen. Diese werden auf höheren Ebenen weiterverarbeitet.  \n",
    "\n",
    "**Beispiel einer einfachen 1D-Faltung:**\n",
    "\n",
    "Die beiden einfachen Beispiele sollen die Berechnung verdeutlichen. Die Filterfunktion wird auf die Pixel gelegt und Elementweise multipliziert. \n",
    "Im folgenden Beispiel werden 3 Pixel eines Bildes verwendet. Die Ergebnisse sagen etwas über den Bildinhalt aus:\n",
    "\n",
    "- positives Ergebnis: Übergang von hell zu dunkel   \n",
    "- negatives Ergebnis: Übergang von dunkel nach hell\n",
    "- neutrales Ergebnis: Übergang wechselnd, hell-dunkel-hell  oder dunkel-hell-dunkel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93daa70-441a-4fa0-967b-6b391f0fc334",
   "metadata": {},
   "source": [
    ":::{figure-md} conv1d-fig\n",
    "<img src=\"images/cnn_1d.png\" alt=\"conv1d\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Eindimensionale Faltung\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b285084-c322-4486-89a5-1b51c6a210cc",
   "metadata": {},
   "source": [
    "Da ein Bild aus mehr als 3 Pixel besteht, muss die Filterfunktion über das gesamte Bild „geschoben“ werden. Das folgende Beispiel demonstriert den Vorgang der Convolution im Fall eines eindimensionalen Filters. Der Filter besteht in diesem Fall wieder aus einem Zeilenvektor mit 3 Elementen. Der Filter wird nun Pixelweise über die Bildzeile geschoben, die Ergebnisse werden gespeichert und geben wiederum Aufschluss über die Bildstruktur.\n",
    "Die Ergebnisse zeigen wieder die enthaltene Bildstruktur: \n",
    "\n",
    "- 1: hell-dunkel\n",
    "- 0: hell-dunkel-hell\n",
    "- 0: dunkel-hell-dunkel\n",
    "- 1: hell-dunkel\n",
    "--1: dunkel-hell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2a353-264e-4cd0-b0cd-570fd13d8849",
   "metadata": {},
   "source": [
    ":::{figure-md} conv1d-fig2\n",
    "<img src=\"images/cnn_1d_long.png\" alt=\"conv1d\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Eindimensionale Faltung mit mehreren Übergängen\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b1c32-d7bc-4295-af74-724dd197cc68",
   "metadata": {},
   "source": [
    "## 2-Dimensionale Faltung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a59ef7-1831-4f2e-bf80-b1024bc8be98",
   "metadata": {},
   "source": [
    "In der Praxis werden in der Bilderkennung 2-dimensionale Filter verwendet, ein häufig verwendetes Format ist ein 3x3 Filter. Der Vorgang ist analog zum eindimensionalen Fall, der Filter wird über das gesamte Bild geschoben. Das folgende Beispiel zeigt einen Filter, der in der Lage ist, senkrechte Kanten zu erkennen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19423a6-4d69-4eea-9e8d-cb39263c9fa0",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"images/cnn_2d_a.png\" alt=\"cnn\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Zweidimensionale Faltung Schrittweise\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c7da6-b46c-40ef-90f1-9d9288aed393",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"images/cnn_2d_b.png\" alt=\"cnn\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Eindimensionale Faltung mit mehreren Übergängen\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4663d5-ada5-4a3c-b1c1-3bbe9df35264",
   "metadata": {},
   "source": [
    "Die Werte der Filter bilden die Gewichte des Convolutional Layer. Diese Gewichte werden durch das Training selbst bestimmt und somit ist das CNN in der Lage, sich selbstständig auf relevante Features wie z.$~$B. Kanten zu fokussieren. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd499995-ca95-43b7-9a67-4fdc64012e5b",
   "metadata": {},
   "source": [
    "**Im Folgenden noch weitere Ergebnisse für bestimmte Bildstrukturen:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77052593-11b7-48fc-a91a-e43b7ed21e0a",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"images/cnn_2d_c.png\" alt=\"pozi\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Zweidimensionale Faltung einer einheitlichen Fläche\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa47c6-dca1-46a3-89b9-9eac0ffb63fc",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"images/cnn_2d_d.png\" alt=\"pozi\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Zweidimensionale Faltung horizontale Kante\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19aa6b-0543-45ba-adf1-088fa78cd21a",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"images/cnn_2d_e.png\" alt=\"cnn\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Zweidimensionale Faltung schräge Kante\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a141745-ce12-4fbe-ac36-2a52cc7235a8",
   "metadata": {},
   "source": [
    "Mit Hilfe der Convolutional-Layer bekommt das neuronale Netz ein „Verständnis“ über die Struktur der Bilder (Ecken, Kanten, usw.) „eingebaut“. Das CNN ist somit auf die Erkennung von Bildern spezialisiert und dementsprechend leistungsfähiger als ein NN ohne dieses Bildverständnis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf2941-2fef-4533-b380-8364ee122bb8",
   "metadata": {},
   "source": [
    "Die Filter oder Kernels gibt man nicht vor, sondern lässt die Werte vom Convolutional Layer ermitteln. Die Kernels werden dabei so bestimmt, dass sie für die Erkennung der Bilder am effektivsten sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4b3f2-0ab0-4081-aad9-db6ca6de6095",
   "metadata": {},
   "source": [
    "Es sollen aber nicht nur vertikale Kanten gefunden werden, sondern auch schräge und waagerechte. Da jeder Filter für ein bestimmtes Feature zuständig ist, benötigt das CNN mehrere solcher Filter, um alle relevanten Zusammenhänge extrahieren zu können. Die Anzahl an Filtern bereitgestellt werden sollten, hängt von den Daten ab und ist ein Hyperparameter den man empirisch ermitteln muss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8499cc-7e86-4121-bc5e-ef990dae7c06",
   "metadata": {},
   "source": [
    "## Convolutional Neural Net mit Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3c6bb73-b765-4ffd-9ad8-bd40d6c856f4",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Vorstellung: MNIST-Daten!\n",
    "# http://yann.lecun.com/exdb/mnist/\n",
    "# FashionMNIST: https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "X_train = load('../02_NN/Dataset/X_train.npy').astype(np.float32)#.reshape(-1, 784)\n",
    "y_train = load('../02_NN/Dataset/y_train.npy')\n",
    "\n",
    "\n",
    "#oh = OneHotEncoder()\n",
    "#y_train_oh = oh.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "\n",
    "X_test=load('../02_NN/Dataset/X_test.npy').astype(np.float32)#.reshape(-1, 784)\n",
    "y_test=load('../02_NN/Dataset/y_test.npy')\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115126e4-5ced-4650-81cc-88f3532b12c6",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 28, 28)\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd6231-4811-4b93-92d4-306a0a566b02",
   "metadata": {},
   "source": [
    "Das Format der Daten (10500, 28, 28) passt noch nicht zum geforderten Eingangsformat und muss angepasst werden. Das CNN verlangt ein vierdimensionales Array (10500, 28, 28, 1) und dieses wird mit der *reshape*-Methode erzeugt (siehe model.fit() im nächsten Programm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd8b25-6c51-4e1e-b1ec-c54c9c052c60",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent**  \n",
    "Das stochastische Gradientenabstiegsverfahren, wird mit *optimizer=\"sgd\"* ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c55a2fb7-2f55-4978-9cf1-ac3e83d1a5f2",
   "metadata": {
    "tags": [
     "scroll-output",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - 1s 22ms/step - loss: 229.5953 - accuracy: 0.3178\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.9440 - accuracy: 0.6190\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.6530 - accuracy: 0.7649\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.5013 - accuracy: 0.8199\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4058 - accuracy: 0.8393\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4397 - accuracy: 0.8294\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3674 - accuracy: 0.8583\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3386 - accuracy: 0.8624\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3027 - accuracy: 0.8836\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3147 - accuracy: 0.8754\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2679 - accuracy: 0.8914\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2448 - accuracy: 0.9017\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2369 - accuracy: 0.9060\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2150 - accuracy: 0.9114\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3123 - accuracy: 0.8828\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2253 - accuracy: 0.9091\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1853 - accuracy: 0.9243\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1935 - accuracy: 0.9216\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1948 - accuracy: 0.9217\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1432 - accuracy: 0.9435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f180005d790>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN!\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "#model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train.reshape(10500,28,28,1),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0406b34-52b7-435d-a0eb-1f51c57c4042",
   "metadata": {},
   "source": [
    "**RMSprop**  \n",
    "\n",
    "Ein weiterer Optimizer ist der RMSprop. Dieser wird durch *optimizer=\"rmsprop\"* ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53a40d7c-2016-42a4-81fe-cfd4b82db2cd",
   "metadata": {
    "tags": [
     "scroll-output",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - 1s 22ms/step - loss: 242.0397 - accuracy: 0.4830\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 63.1420 - accuracy: 0.6419\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 27.1203 - accuracy: 0.7310\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 10.0603 - accuracy: 0.8360\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 4.6834 - accuracy: 0.8353\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 2.2660 - accuracy: 0.8735\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 1.4034 - accuracy: 0.9078\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.9845 - accuracy: 0.9339\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.5971 - accuracy: 0.9523\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0607 - accuracy: 0.9875\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4934 - accuracy: 0.9664\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0084 - accuracy: 0.9981\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.6108 - accuracy: 0.9581\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 3.7297e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4393 - accuracy: 0.9813\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 5.3599e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 2.5452e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3049 - accuracy: 0.9837\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 4.4494e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17e27e5e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN!\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train.reshape(10500,28,28,1),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4e229-0dca-4a8c-9912-556f062f2a4b",
   "metadata": {},
   "source": [
    "**2 Convolutional Layer**\n",
    "\n",
    "Einen weiteren Convolutional Layer verwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b708e2e1-db14-424d-8f69-059a20aa0044",
   "metadata": {
    "tags": [
     "scroll-output",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - 2s 96ms/step - loss: 21.6524 - accuracy: 0.6189\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - 2s 95ms/step - loss: 0.8093 - accuracy: 0.8665\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - 2s 95ms/step - loss: 0.4227 - accuracy: 0.8997\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - 2s 94ms/step - loss: 0.1646 - accuracy: 0.9585\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - 2s 98ms/step - loss: 0.0131 - accuracy: 0.9990\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - 2s 96ms/step - loss: 0.2447 - accuracy: 0.9744\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - 2s 96ms/step - loss: 0.0067 - accuracy: 0.9998\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - 2s 95ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - 2s 118ms/step - loss: 0.0815 - accuracy: 0.9895\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - 3s 127ms/step - loss: 0.0597 - accuracy: 0.9820\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - 3s 127ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - 3s 126ms/step - loss: 3.9080e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - 3s 128ms/step - loss: 1.1759e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - 3s 127ms/step - loss: 0.0034 - accuracy: 0.9983\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - 3s 126ms/step - loss: 0.1700 - accuracy: 0.9794\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - 3s 125ms/step - loss: 2.5466e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - 3s 128ms/step - loss: 1.7362e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - 3s 132ms/step - loss: 6.7044e-05 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - 3s 128ms/step - loss: 7.7813e-05 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - 3s 132ms/step - loss: 0.0971 - accuracy: 0.9824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17e27a5150>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN!\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train.reshape(10500,28,28,1),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94a62b-3293-4ad0-bf95-3d60aaf3b14f",
   "metadata": {},
   "source": [
    "Die Schraubenkopfbilder können vom CNN sehr gut gelernt werden. Die Trainingsgenauigkeit beträgt bis zu 100 %. Das war zu erwarten, da die Bilder recht einfach zu unterscheiden sind und für ein CNN kein Problem darstellen sollten. \n",
    "\n",
    "Im nächsten Abschnitt wird der Datensatz mit den echten Bildern verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2eced-12e4-4621-ab4f-69e724aaafb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
