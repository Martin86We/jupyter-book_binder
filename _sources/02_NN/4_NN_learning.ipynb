{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab69a301-032e-4272-88d3-ec3fe6bc87f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "(nn_learning)=\n",
    "# Neuronales Netz (Funktionsweise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f672a1b-48bd-47bc-8ac8-33ec0959a881",
   "metadata": {},
   "source": [
    "In diesem Abschnitt soll ein Einblick in den Aufbau und den Lernvorgang eines neuronalen Netzes geschaffen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f56ee-842d-4ff4-b355-0154d9d4347d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Verdeckte Schicht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec39667-1cc6-42b9-991a-fb8ed299a142",
   "metadata": {},
   "source": [
    "Bisher hatten wir nur ein Neuron. Da ein neuronales Netz aus mehreren solcher Neuronen aufgebaut ist, befasst sich dieser Abschnitt damit, wie die einzelnen Neuronen zu einem Netz verknüpft werden können, wie diese Neuronen arbeiten und wie es das neuronale Netz schafft, etwas zu lernen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01623d0-ba5f-4a05-9f9f-9d88217c2f95",
   "metadata": {},
   "source": [
    "**Ein Netz aus mehreren Neuronen:**\n",
    "Wir beginnen wieder mit einem einfachen Beispiel und verbinden ein paar Neuronen zu einem einfach NN:\n",
    "\n",
    "- **Input Layer:** X1, X2, X3, b\n",
    "- **Hidden Layer** Neuron 1, Neuron 2, Neuron 3\n",
    "- **Output Layer** Neuron 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2a843-55d5-44e4-9756-b66e46b548bf",
   "metadata": {},
   "source": [
    "**Beispiel:**\n",
    "- X1: Anzahl Zylinder\n",
    "- X2: Leistung kw\n",
    "- X3: Gewicht kg\n",
    "\n",
    "\n",
    "Die Neuronen verteilen sich selbst auf verschiedene Features auf. \n",
    "Jedes Neuron spezialisiert sich auf eine bestimmte Eigenschaft z.B.:\n",
    "\n",
    "- Neuron 1: Kleinwagen oder SUV (relevant: X1, X2, X3)\n",
    "- Neuron 2: Preis\n",
    "- Neuron 3: Beschleunigung (relevant: X2,X3)\n",
    "\n",
    "Relevante Verbindungen werden vom Algorithmus verstärkt und nicht benötigte Verbindungen werden ignoriert (Gewicht wird sehr klein oder Null).\n",
    "\n",
    "Der Output-Layer soll z.B. den Verbrauch vorhersagen und wird entsprechend jene Verbindungen verstärken, die besonders großen Einfluss auf den Verbrauch haben.\n",
    "\n",
    "Die Aktualisierung der Gewichte hat einen großen Einfluss auf diesen Vorgang."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b4a75d-c98f-4a3d-9c28-986f77eefd2c",
   "metadata": {},
   "source": [
    ":::{figure-md} two layer net\n",
    "<img src=\"images/hiddenLayer_1.png\" alt=\"nn\" class=\"bg-primary mb-1\" width=\"1000px\">\n",
    "\n",
    "zweilagiges-Neuronales Netz.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f9a34-268a-42cd-8016-0ada628508b9",
   "metadata": {},
   "source": [
    "Weitere Informationen:\n",
    "[Neural network architecture](https://otexts.com/fpp2/nnetar.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccdaeb4-a2be-4a0a-a65c-286498930bc8",
   "metadata": {},
   "source": [
    ":::{note} Die Neuronen im Hidden Layer übernehmen jeweils verschiedene Hilfsaufgaben. Der Output Layer kombiniert der Ergebnisse aus dem Hidden Layer und gibt eine Vorhersage aus.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8194f4-43e6-41cd-86db-9bd6aba36244",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "**Es gilt grundsätzlich:**\n",
    "- Neuronale Netze mit einem beliebig großen Hidden-Layer können jede beliebige Funktion approximieren.\n",
    "- je mehr Knoten das Netz besitzt, desto genauer kann es die math. Funktionen annähern.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f672cfca-876d-41c7-913f-1308c02d472f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gewichte aktualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f50af31-20c8-4f4a-b5c4-92ab70374917",
   "metadata": {},
   "source": [
    "Nach der Initialisierung der Gewichte wird ein Ausgangswert berechnet. Passt dieser \"Vorhersagewert\" nicht zum richtigen Ergebnis, dann müssen die Gewichte dementsprechend angepasst werden, so dass das Ergebnis stimmt.\n",
    "\n",
    "In der Abb.[Link] werden w1 und w2 so lange erhöht, bis der Vorhersagewert zum richtigen Wert passt.\n",
    "\n",
    "Das Neuron gibt 0.5 aus, obwohl der richtige Wert 0.75 ist. Das bedeutet, das Modell ist noch nicht so gut an die Daten angepasst. Um das Modell den Daten besser anzupassen, stehen nur die Gewichte als Stellschrauben zur Verfügung und diese können nun stückweise erhöht werden bis das Modell die Daten ausreichend approximiert hat, siehe [Link].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bbefea-33a1-4407-b09d-7e30ec9a59a8",
   "metadata": {},
   "source": [
    ":::{figure-md} two layer net\n",
    "<img src=\"images/weights_increase.png\" alt=\"nn\" class=\"bg-primary mb-1\" width=\"600px\">\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968248c-b5e5-454c-9ecf-c1664c90a13c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kostenfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9c5bb-b9f2-41a7-844b-8db4be26c866",
   "metadata": {},
   "source": [
    "Die Aktualisierung der Gewichte wird mit Hilfe einer Kostenfunktion erreicht. Es gibt verschiedene Kostenfunktionen, eine davon ist die **\"quadratische Fehlerfunktion\"**.\n",
    "\n",
    "Weitere Kostenfunktionen und Informationen [hier](https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5553753-5fd6-4749-8889-a866fc0451ad",
   "metadata": {},
   "source": [
    ":::{figure-md} two layer net\n",
    "<img src=\"images/weights_1.png\" alt=\"nn\" class=\"bg-primary mb-1\" width=\"750px\">\n",
    "\n",
    "Kostenfunktion\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecd5306-ba59-4c41-9517-f569709c18b8",
   "metadata": {},
   "source": [
    "Das Quadrieren des Fehlers in der Kostenfunktion bewirkt eine viel größere Bestrafung für größere Fehler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f002e-7471-4a30-82f3-f4e9828ff4e8",
   "metadata": {},
   "source": [
    "Werden nun wie üblich mehrere Datensätze trainiert, müssen die Gewichte nach jedem Datensatz angepasst werden, solange die Vorhersage vom wahren Wert y abweicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9b564-45f0-420e-997e-51f16e03aad2",
   "metadata": {},
   "source": [
    ":::{figure-md} two layer net\n",
    "<img src=\"images/weights_cost.png\" alt=\"nn\" class=\"bg-primary mb-1\" width=\"1000px\">\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f766b70-b314-42b7-aea9-2c9fb4d37bae",
   "metadata": {},
   "source": [
    "Die Abbildung dient nur zur Veranschaulichung. Im nächsten Abschnitt (Gradientenabstieg) wird gezeigt, wie die Kostenfunktion in Python minimiert wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86db40b-0b8b-4a32-a541-88804bc59b75",
   "metadata": {},
   "source": [
    "Eine Kostenfunktion dient zum Minimieren des Fehlers. Man stellt eine Funktion auf, die den Fehler zwischen Schätzung und richtigem Wert berechnet und sucht dann die zugehörigen Gewichte, die den Fehler minimal werden lassen. Die Kosten C werden als Funktion der Gewichte formuliert. Da man es bei NN’s in der Regel mit komplexeren Funktionen und sehr vielen Gewichten zu tun hat, lässt sich das nicht mehr analytisch lösen. Für so einen Fall eignet sich das Gradientenabstiegsverfahren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc730af7-2e05-469b-852c-b66fe6f58152",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradientenabstieg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33e92e-6114-49e7-b30d-81d780f651be",
   "metadata": {},
   "source": [
    "Wichtig zum Verständnis des Trainings von neuronalen Netzen.\n",
    "\n",
    "Wie aktualisiert der Computer mehrere tausend Gewichte?\n",
    "\n",
    "Mit dem Gradientenabstiegsverfahren wird Schritt für Schritt das Minimum einer Funktion gesucht. Bei einfachen Funktionen kann man das noch analytisch lösen, aber bei komplexeren Funtionen benötigt man das Gradientenabstiegsverfahren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e934c6-746a-419b-89d7-9087a15544a1",
   "metadata": {},
   "source": [
    "Wie findet man das Minimum bei komplexen Funktionen wie im Bild [Link]?\n",
    "Die Antwort lautet: Gradientenabstiegsverfahren. Das Verfahren wird anhand eines einfachen Beispiels näher erläutert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df81dba-2442-46fa-aaf1-7c5d12be6575",
   "metadata": {
    "tags": []
   },
   "source": [
    ":::{figure-md} gradient_descent\n",
    "<img src=\"images/gradient_descent.png\" alt=\"nn\" class=\"bg-primary mb-1\" width=\"500px\">\n",
    "\n",
    "Gradientenabstiegsverfahren.[[Quelle](https://www.researchgate.net/profile/Alexander-Amini/publication/325142728/figure/fig1/AS:766109435326465@1559666131320/Non-convex-optimization-We-utilize-stochastic-gradient-descent-to-find-a-local-optimum.jpg)]\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec4c31-433f-401b-895b-2ec012ce691d",
   "metadata": {
    "tags": [
     "output_scroll",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x ** 2 - 4 * x + 5\n",
    "\n",
    "\n",
    "def f_ableitung(x):\n",
    "    return 2 * x - 4\n",
    "\n",
    "\n",
    "x = 5\n",
    "\n",
    "#Schrittweite bzw Lernrate (lr):\n",
    "lr = 0.05\n",
    "\n",
    "plt.scatter(x, f(x), c=\"r\")\n",
    "for i in range(0, 25):\n",
    "    steigung_x = f_ableitung(x)\n",
    "    x = x - lr * steigung_x\n",
    "    plt.scatter(x, f(x), c=\"r\")\n",
    "    print(x)\n",
    "\n",
    "xs = np.arange(-2, 6, 0.1)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76be1d-2853-4bf2-a45a-1d60190dfc57",
   "metadata": {},
   "source": [
    ":::{important}\n",
    "Man kann nun mit der Lernrate experimentieren und z.$~$B. einen großen Wert wählen. Es kann passieren, dass bei einer zu großen Schrittweite das Minimum übersprungen wird und somit nicht gefunden werden kann.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895749f1-b25e-4619-bfce-5e27153dac99",
   "metadata": {},
   "source": [
    "```{admonition} Gut zu wissen\n",
    ":class: tip\n",
    "In hochdimensionalen Räumen, spielen Probleme mit lokalen Minima kaum eine Rolle. Bei zwei und drei Dimensionen sind lokale Minima üblich. Bei einer Million Dimensionen sind lokale Minima selten. Die intuitive Erklärung ist, dass die Funktion, damit ein lokales Minimum existiert, in jeder Dimension gleichzeitig nach oben gekrümmt sein muss (erste Ableitung = 0, zweite Ableitung >= 0). Es macht Sinn, dass dies immer weniger wahrscheinlich wird, wenn weitere Dimensionen hinzugefügt werden, und bei einer Million Dimensionen ist es verschwindend unwahrscheinlich.\n",
    "Was Sie anstelle lokaler Minima erhalten, sind Sattelpunkte, an denen sich einige Dimensionen nach oben und andere nach unten krümmen. Sattelpunkte können ebenfalls problematisch für die Optimierung sein, aber sie können mit ausgefallenen Optimierungstechniken behandelt werden.[Quelle](https://news.ycombinator.com/item?id=10678121)  \n",
    "Eine genauere Erklärung finden Sie unter  [Link](http://arxiv.org/abs/1406.2572)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f20ba6-3c4f-46f5-b78a-9f60c14fbc69",
   "metadata": {},
   "source": [
    "In der Praxis hat man es eher mit komplexeren Funktionen mit mehreren Minima zu tun. Die Gefahr, in einem lokalen Minimum stecken zu bleiben, ist in höher-dimensionalen Räumen zu vernachlässigen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395785ca-1381-4510-b962-1a2a73fabb69",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dca980-06d6-486d-b9e1-12b31e91bc89",
   "metadata": {},
   "source": [
    "Die Kosten für die gesamten Trainingsdaten zu berechnen und anschließend die Gewichte zu aktualisieren, würde bei sehr vielen Gewichten einen sehr hohen Rechenaufwand bedeuten, da die Kostenfunktion dann sehr viele variable Gewichte enthält. Deswegen geht man bei NN’s so vor, dass man nicht die Kosten für die gesamten Daten, sondern nur für den einzelnen Batch berechnet und somit die Kosten approximiert. Das macht man dann für alle Batches und aktualisiert nach jedem Batch die Gewichte. So werden die Gewichte pro kompletten Durchgang mehrmals aktualisiert und nicht nur einmal am Ende eines kompletten Durchgangs. Das bringt allerdings mit sich, dass die Gewichte hin und her springen, im „ZickZack zum Minimum laufen“, das führt insgesamt zu einem schnelleren Lernvorgang."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181590ae-299e-4a8d-ad8a-15c3e80497f6",
   "metadata": {},
   "source": [
    "**Begriffsdefinition:**\n",
    "- Batch: Eine Gruppe von Trainingsdaten innerhalb des Datensatzes\n",
    "- Epoche: Alle Batches wurden einmal durchlaufen\n",
    "- Lernrate: Schrittgröße"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0204ab2-6bea-4ad8-80ca-e5dde32f04b9",
   "metadata": {},
   "source": [
    ":::{figure-md} two layer net\n",
    "<img src=\"images/gradient_batch1.png\" alt=\"nn\" class=\"bg-primary mb-1\" width=\"1000px\">\n",
    "\n",
    "erste Batch\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a943b26-d68e-4992-b1b3-8b361bebb49b",
   "metadata": {},
   "source": [
    "Anstatt alle Gewichte mit einmal zu bestimmen, ist es vorteilhafter, den Trainingssatz in einzelne Batches aufzuteilen. Somit wird die Berechnung schneller und die Gewichte werden nach jedem Durchlauf angepasst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7637d75-8d5f-47db-b81f-50bf2b8db718",
   "metadata": {},
   "source": [
    "Ablauf der Gewichtsanpassung für einen Batch:\n",
    "- Vorhersage machen\n",
    "- Kosten berechnen\n",
    "- Gewichte anpassen\n",
    "- dann nächste Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4d28d-afca-469c-a69b-c3a0421dceed",
   "metadata": {},
   "source": [
    ":::{figure-md} two layer net\n",
    "<img src=\"images/gradient_batch2.png\" alt=\"nn\" class=\"bg-primary mb-1\" width=\"1000px\">\n",
    "\n",
    "Zweite Batch\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e6db3-3feb-44a2-8387-67eb269add3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59e4ae-b65e-4601-bfb0-4070374b40ea",
   "metadata": {},
   "source": [
    "Problematik beim Lernen von mehrschichtigen NN's:\n",
    "- Wie werden die Gewichte einer vorherigen Schicht aktualisiert?\n",
    "\n",
    "Die Antwort lautet: Backpropagation!\n",
    "\n",
    "Neuronale Netze wurden erst durch Backpropagation leistungsstark. Dadurch erst war es möglich, das gesamte neuronale Netz zu trainieren, also auch die vorherigen Schichten. Die Gewichte des gesamten NN werden immer wieder aktualisiert, solange bis der Vorhersagewert so nah wie möglich am gewünschten Wert liegt. Wie das genau gemacht wird, soll in diesem Abschnitt gezeigt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78448bb-1e0a-480c-b86f-1627efc0018c",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "- verleiht den NN's ihre Leistungsfähigkeit\n",
    "- verhalf zum Durchbruch von NN's\n",
    "- Idee aus den 70ern\n",
    "- vorher konnte man nur Teile eines Netzes trainieren\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0219e86-2a62-4534-98ee-93cc191819b5",
   "metadata": {},
   "source": [
    "**Ein grobes, einfaches Beispiel zur Backward-Propagation**:\n",
    "\n",
    "Mit einem einfachen, groben Beispiel soll der Einstieg in das Verständnis der Backpropagation erleichtert werden. Für den Einstieg wird auf ein grobes Rechenbeispiel zurückgegriffen. Es soll an dieser Stelle zunächst nur der grobe Vorgang der Backward-Propagation veranschaulicht werden.\n",
    "\n",
    "- Es wird eine Vorhersage mit dem NN gemacht, diese Vorhersage $\\hat{y}$, weicht vom gewünschten / wahren Wert y ab\n",
    "- Die Abweichung e (Error) ist ein Maß dafür, wie stark die Vorhersage vom wahren Wert abweicht\n",
    "- Um die Abweichung zu minimieren, müssen nun die Gewichte aktualisiert werden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a7d6cd-3c58-4da4-94aa-ae894a278111",
   "metadata": {},
   "source": [
    "Der Fehler **e=2** wird an die Ausgänge des Hidden-Layer transformiert:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49694a-3e1c-4468-832a-6f985edfdef8",
   "metadata": {},
   "source": [
    ":::{figure-md} backprop1\n",
    "<img src=\"images/backprop1.png\" alt=\"nn\" class=\"bg-primary mb-1\" width=\"1000px\">\n",
    "\n",
    "Backpropagation\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f2caa-0e03-4cab-adfb-54df3cf76d85",
   "metadata": {},
   "source": [
    "**Initialisierung der Gewichte**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84022e70-3e0e-4dde-bcc9-369a63971d78",
   "metadata": {},
   "source": [
    "Die Initialisierung kann darüber entscheiden, ob das NN trainieren kann oder nicht.\n",
    "Wie initialisieren wir die Gewichte? Mit null, wie im rechten Bild? Die Neuronen würden nur Nullen ausgeben. Das ergibt also keinen Sinn. Doch welche Werte sollte man am besten wählen? \n",
    "Weiterhin dürfen die Gewichte nicht alle mit den gleichen Werten initialisiert werden. Das ist auch aktiver Forschungsgegenstand, denn bei mehrschichtigen Netzen wird es umso wichtiger, die Gewichte „richtig“ bzw. nicht komplett falsch zu wählen. \n",
    "Besser ist es, den Gewichten unterschiedliche Werte zu geben. Das können zufällige, eher kleine Werte sein. So ist sichergestellt, dass jedes Neuron eine andere Funktion berechnet. Somit kann dann auch die Backpropagation richtig arbeiten und die Gewichte anpassen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b717ab8-b0d2-4a26-b979-3a47b7ba7fed",
   "metadata": {},
   "source": [
    ":::{figure-md} backprop1\n",
    "<img src=\"images/backprop2.png\" alt=\"nn\" class=\"bg-primary mb-1\" width=\"1000px\">\n",
    "\n",
    "Initialisierung der Gewichte\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa57e8-480a-49bd-b2bb-d63f9a42c500",
   "metadata": {},
   "source": [
    ":::{figure-md} backprop1\n",
    "<img src=\"images/backprop3.png\" alt=\"nn\" class=\"bg-primary mb-1\" width=\"1000px\">\n",
    "\n",
    "Initialisierung der Gewichte mit kleinen Werten\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba2d48-71d2-4ce1-8a49-2e0cdc6016d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
